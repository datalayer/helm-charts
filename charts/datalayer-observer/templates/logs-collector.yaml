{{- if index .Values "opentelemetry-operator" "enabled" }}
apiVersion: opentelemetry.io/v1beta1
kind: OpenTelemetryCollector
metadata:
  name: datalayer-logs-collector
spec:
  # The opentelemetry-collector chart is a good place to look for good configuration
  # Reference: https://github.com/open-telemetry/opentelemetry-helm-charts/tree/main/charts/opentelemetry-collector
  env:
  - name: KUBE_NODE_NAME
    valueFrom:
      fieldRef:
        apiVersion: v1
        fieldPath: spec.nodeName
  mode: daemonset
  config:
    # Configuration inspired by https://grafana.com/docs/grafana-cloud/monitor-infrastructure/kubernetes-monitoring/configuration/configure-infrastructure-manually/otel-collector/
    # https://opentelemetry.io/docs/kubernetes/collector/components/#filelog-receiver
    receivers:
      filelog:
        include:
          - /var/log/pods/*/*/*.log
        exclude:
          # Exclude collector container's logs. The file format is /var/log/pods/<namespace_name>_<pod_name>_<pod_uid>/<container_name>/<run_id>.log
          # This is important if the debug exporter is set to avoid log explosion.
          - /var/log/pods/{{ default .Values.observer.namespace .Release.Namespace }}_datalayer-logs-collector*_*/otc-container/*.log
        include_file_path: true
        include_file_name: false
        operators:
          # parse container logs
          - type: container
            id: container-parser
            max_log_size: 102400
          - type: add
            field: resource["k8s.cluster"]
            value: '{{ .Values.logsCollector.clusterName }}'

    processors:
      # https://github.com/open-telemetry/opentelemetry-collector/tree/main/processor/batchprocessor
      batch: {}
      # https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/k8sattributesprocessor
      k8sattributes:
        extract:
          metadata:
          - k8s.namespace.name
          - k8s.deployment.name
          - k8s.statefulset.name
          - k8s.daemonset.name
          - k8s.cronjob.name
          - k8s.job.name
          - k8s.node.name
          - k8s.pod.name
          - k8s.pod.uid
          - k8s.pod.start_time
          labels:
          - tag_name: app.name
            key: datalayer.io/app
            from: pod
          - tag_name: datalayer.environment.name
            key: environment.datalayer.io/name
            from: pod
          - tag_name: datalayer.pool.name
            key: jupyterpool.datalayer.io/name
            from: pod
          - tag_name: datalayer.pool.status
            key: jupyterpool.datalayer.io/pod-status
            from: pod
          - tag_name: datalayer.pool.user
            key: jupyterpool.datalayer.io/user-uid
            from: pod
          - tag_name: datalayer.pool.started-at
            key: jupyterpool.datalayer.io/started-at
            from: pod
          - tag_name: datalayer.pool.expired-at
            key: jupyterpool.datalayer.io/expired-at
            from: pod
          - tag_name: datalayer.pool.type
            key: jupyterpool.datalayer.io/kernel-type
            from: pod
          - tag_name: datalayer.pool.reservation
            key: jupyterpool.datalayer.io/reservation-id
            from: pod
        filter:
          node_from_env_var: KUBE_NODE_NAME
        passthrough: false
        pod_association:
        - sources:
          - from: resource_attribute
            name: k8s.pod.ip
        - sources:
          - from: resource_attribute
            name: k8s.pod.uid
        - sources:
          - from: connection
      # https://github.com/open-telemetry/opentelemetry-collector/tree/main/processor/memorylimiterprocessor
      # Default memory limiter configuration for the collector based on k8s resource limits.
      memory_limiter:
        # check_interval is the time between measurements of memory usage.
        check_interval: 5s
        # By default limit_mib is set to 80% of ".Values.resources.limits.memory"
        limit_percentage: 80
        # By default spike_limit_mib is set to 25% of ".Values.resources.limits.memory"
        spike_limit_percentage: 25

    exporters:
      # https://github.com/open-telemetry/opentelemetry-collector/tree/main/exporter/debugexporter
      debug:
        verbosity: detailed

      # Data sources: traces, metrics, logs
      # https://github.com/open-telemetry/opentelemetry-collector/tree/main/exporter/otlphttpexporter
      otlphttp:
        logs_endpoint: http://{{ default .Values.observer.namespace .Release.Namespace }}-loki.{{ default .Values.observer.namespace .Release.Namespace }}.svc.cluster.local:3100/otlp/v1/logs

    extensions:
      # https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/extension/healthcheckextension
      health_check: {}
      # https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/extension/pprofextension
      pprof: {}
      # https://github.com/open-telemetry/opentelemetry-collector/blob/main/extension/zpagesextension/README.md
      zpages: {}

    service:
      extensions: [health_check, pprof, zpages]
      pipelines:
        logs:
          receivers: [filelog]
          processors: [k8sattributes, memory_limiter, batch]
          exporters: [otlphttp]

  # Mount node folders to access logs within the daemonset containers
  volumeMounts:
  - mountPath: /var/log
    name: varlog
    readOnly: true
  - mountPath: /var/lib/docker/containers
    name: varlibdockercontainers
    readOnly: true
  volumes:
  - name: varlog
    hostPath:
      path: /var/log
  - name: varlibdockercontainers
    hostPath:
      path: /var/lib/docker/containers
{{- end }}
